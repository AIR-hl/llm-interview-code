{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2280cbc6",
   "metadata": {},
   "source": [
    "# BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ee9392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}\n",
    "        self.merges = {} # 合并规则，{key: (token1, token2), value: merged_token}\n",
    "\n",
    "    def _get_stats(self, vocab):\n",
    "        \"\"\"\n",
    "        统计所有相邻 token 对的出现频率\n",
    "        :param vocab: 当前的语料库词汇表，格式为 {'l o w </w>': 5, ...}\n",
    "        :return: 一个 Counter 对象，记录了每个 token 对的频率\n",
    "        \"\"\"\n",
    "        pairs = Counter()\n",
    "        for word, freq in vocab.items():\n",
    "            tokens = word.split()\n",
    "            # 遍历单词中的所有相邻 symbol 对\n",
    "            for i in range(len(tokens) - 1):\n",
    "                pairs[tokens[i], tokens[i+1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    def _merge_vocab(self, pair, v_in):\n",
    "        \"\"\"\n",
    "        在词汇表中执行一次合并操作\n",
    "        :param pair: 需要合并的 token 对，例如 ('e', 's')\n",
    "        :param v_in: 输入的词汇表\n",
    "        :return: 合并后的新词汇表\n",
    "        \"\"\"\n",
    "        v_out = {}\n",
    "        bigram = re.escape(' '.join(pair)) # 将 ('e', 's') 拼接成 'e s'，用于在字符串中查找\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') # 替换模式：查找独立的 'e s' 对\n",
    "        \n",
    "        for word in v_in:\n",
    "            # 将 'e s' 替换为 'es'\n",
    "            w_out = p.sub(''.join(pair), word)\n",
    "            v_out[w_out] = v_in[word]\n",
    "        return v_out\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        \"\"\"\n",
    "        训练 BPE 模型\n",
    "        corpus: 文本语料\n",
    "        \"\"\"\n",
    "        # 1. 初始化预分词词汇表\n",
    "        #    将 'lowest' 变为 'l o w e s t </w>'，</w> 是bpe中的特殊词尾符号\n",
    "        word_counts = Counter(corpus.split())\n",
    "        vocab = {' '.join(word) + ' </w>': freq for word, freq in word_counts.items()}\n",
    "\n",
    "        # 2. 获取初始词表（所有单个字符）\n",
    "        alphabet = set()\n",
    "        for word in vocab:\n",
    "            alphabet.update(list(word.split()))\n",
    "        \n",
    "        # 初始词表就是这些基本字符\n",
    "        self.vocab = {char: i for i, char in enumerate(alphabet)}\n",
    "        \n",
    "        num_merges = self.vocab_size - len(self.vocab)\n",
    "        for i in range(num_merges):\n",
    "            # 统计当前词汇表中所有相邻 token 对的频率\n",
    "            pairs = self._get_stats(vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            # 找到频率最高的 token 对\n",
    "            most_pair = pairs.most_common()[0][0]\n",
    "            vocab = self._merge_vocab(most_pair, vocab)\n",
    "            \n",
    "            merged_token = ''.join(most_pair)\n",
    "            self.merges[most_pair] = merged_token\n",
    "            \n",
    "            if merged_token not in self.vocab:\n",
    "                self.vocab[merged_token] = len(self.vocab)\n",
    "        \n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        将输入的文本字符串进行分词\n",
    "        :param text: 待分词的单词或句子，例如 \"lowest\"\n",
    "        :return: token 列表\n",
    "        \"\"\"\n",
    "        # 预处理：将单词拆分为字符，并添加词尾符号\n",
    "        words=text.split()\n",
    "        all_token_ids = []\n",
    "\n",
    "        for word in words:\n",
    "            tokens = list(word)\n",
    "            tokens = ' '.join(tokens) + ' </w>'\n",
    "            tokens = tokens.split()\n",
    "            \n",
    "            # 获取所有可能的 token 对\n",
    "            def get_pairs(symbols):\n",
    "                pairs = set()\n",
    "                for i in range(len(symbols) - 1):\n",
    "                    pairs.add((symbols[i], symbols[i+1]))\n",
    "                return pairs\n",
    "\n",
    "            while True:\n",
    "                pairs = get_pairs(tokens)\n",
    "                # 寻找在当前文本中可以合并的、优先级最高的（粒度最细，最早学会的）合并规则\n",
    "                # 注意：这里需要按 self.merges 的学习顺序来查找，因为它是带优先级的\n",
    "                best_pair_to_merge = None\n",
    "                for pair in self.merges:\n",
    "                    if pair in pairs:\n",
    "                        best_pair_to_merge = pair\n",
    "                        break # 找到第一个（优先级最高）就跳出\n",
    "                \n",
    "                if best_pair_to_merge is None:\n",
    "                    break\n",
    "                \n",
    "                # 执行合并\n",
    "                first, second = best_pair_to_merge\n",
    "                new_tokens = []\n",
    "                i = 0\n",
    "                while i < len(tokens):\n",
    "                    if i < len(tokens) - 1 and tokens[i] == first and tokens[i+1] == second:\n",
    "                        new_tokens.append(first + second)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_tokens.append(tokens[i])\n",
    "                        i += 1\n",
    "                tokens = new_tokens\n",
    "            all_token_ids.extend([self.vocab[v] for v in tokens])\n",
    "        return all_token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c751dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter=Counter()\n",
    "counter.update([\"Hello world\"])\n",
    "counter.update([\"Hello world\"])\n",
    "counter.update([\"Hello world\"])\n",
    "counter.update([\"Hello\"])\n",
    "counter.update([\"Hello wo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "820ff5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world 3\n"
     ]
    }
   ],
   "source": [
    "for item, freq in counter.items():\n",
    "    print(item, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21ab0342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Hello world', 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1528c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
