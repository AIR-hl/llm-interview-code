{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1defb7bf",
   "metadata": {},
   "source": [
    "# RoPE\n",
    "\n",
    "$$\n",
    "x=[x^{(0)},x^{(1)},...,x^{(|D|-1)}]\n",
    "$$\n",
    "$$\n",
    "f_{rope}([x^{(2d)},x^{2d+1}]^T)=\\begin{pmatrix}  \\cos m\\theta_d & -\\sin m\\theta_d) \\\\  \\sin m \\theta_d &  \\cos m \\theta_d \\end{pmatrix}\\begin{pmatrix}  x^{(2d)}  \\\\  x^{2d+1}    \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4022f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class RoPEEmbedding(nn.Module):\n",
    "    def __init__(self, head_dim, max_seq_len, base=10000):\n",
    "        super().__init__()\n",
    "        assert head_dim % 2==0, \"维度必须为偶数\"\n",
    "\n",
    "        self.head_dim=head_dim\n",
    "        self.max_seq_len=max_seq_len\n",
    "        self.base=base\n",
    "\n",
    "        # 计算  theta = 1 / (base^(2i / head_dim))\n",
    "        theta=1.0 / (base**(torch.range(0, head_dim, 2).float() / head_dim)) \n",
    "        \n",
    "        pos_ids=torch.arrange(max_seq_len)\n",
    "        freqs=pos_ids * theta\n",
    "        sin = torch.sin(freq)\n",
    "        cos = torch.cos(freq)\n",
    "        self.register_buffer('sin_table', sin)  # [max_seq_len, head_dim/2]\n",
    "        self.register_buffer('cos_table', cos)  # [max_seq_len, head_dim/2]\n",
    "\n",
    "    def forward(self, x, offset=0):\n",
    "        _, _, seq_len, _=x.shape # [batch_size, num_heads, seq_len, head_dim]\n",
    "\n",
    "        sin=self.sin_table[offset:seq_len+offset]\n",
    "        cos=self.cos_table[offset:seq_len+offset]\n",
    "\n",
    "        x1=x[..., 0::2] # [batch_size, num_heads, seq_len, head_dim//2]\n",
    "        x2=x[..., 1::2]\n",
    "        rotated_x1=x1*cos - x2*sin\n",
    "        rotated_x2=x2*cos + x1*sin\n",
    "        # 使用 stack 和 flatten/reshape 来高效地交错合并\n",
    "        # 1. 堆叠: [batch_size, num_heads, seq_len, head_dim / 2, 2]\n",
    "        # 2. 展平: [batch_size, num_heads, seq_len, head_dim]        \n",
    "        rotated_x = torch.stack((rotated_x1, rotated_x2), dim=-1).flatten(-2)\n",
    "        return rotated_x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
