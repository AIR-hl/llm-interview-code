{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eabe5653",
   "metadata": {},
   "source": [
    "# GRPO\n",
    "\n",
    "$$\n",
    "\\mathcal{J}_{\\text{GRPO}}(\\theta) =  \\mathbb{E}_{q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text{old}}}(O \\mid q)} \\Bigg[ \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\min \\Bigg( \\frac{\\pi_\\theta(o_{i,t} \\mid q, o_{i,<t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t} \\mid q, o_{i,<t})} \\hat{A}_{i,t}, \\nonumber  \\text{clip} \\left( \\frac{\\pi_\\theta(o_{i,t} \\mid q, o_{i,<t})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t} \\mid q, o_{i,<t})}, 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A}_{i,t} \\Bigg) \\nonumber - \\beta D_{\\text{KL}}[\\pi_\\theta \\| \\pi_{\\text{ref}}] \\Bigg],\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{A}_{i,t}=\\tilde{r}_i=\\frac{r_i-\\text{mean}(\\mathbf{r})}{\\text{std}(\\mathbf{r})} \\notag\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d09cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def grpo_loss(rewards, logp_per_token, ref_logp_per_token, old_logp_per_token, beta=0.01 , clip_epsilon=0.25):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        rewards (torch.Tensor): 奖励, shape: [batch_size, num_generation]\n",
    "        logp_per_token (torch.Tensor): 策略模型logp, shape: [batch_size, num_generation, seq_len]\n",
    "        ref_logp_per_token (torch.Tensor): 参考模型logp, shape: [batch_size, num_generation, seq_len]\n",
    "        old_logp_per_token (torch.Tensor): 旧策略模型logp, shape: [batch_size, num_generation, seq_len]\n",
    "        beta (float): KL正则化参数\n",
    "        clip_epsilon (float): 裁剪参数\n",
    "    \"\"\"\n",
    "    mean_grouped_rewards = rewards.mean(dim=-1, keepdim=True) # shape: [batch_size, 1]\n",
    "    std_grouped_rewards = rewards.std(dim=-1, keepdim=True)   # shape: [batch_size, 1]\n",
    "    \n",
    "    advantage_per_sequence = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-8) # [batch_size, num_generation]\n",
    "    advantage_per_token = advantage_per_sequence.unsqueeze(-1).expand_as(logp_per_token) # [batch_size, num_generation, seq_len]\n",
    "    \n",
    "    importance_ratio = torch.exp(logp_per_token - old_logp_per_token)\n",
    "    clipped_importance_ratio = torch.clamp(importance_ratio, 1.0 - clip_epsilon, 1.0 + clip_epsilon)\n",
    "\n",
    "    adv1 = importance_ratio * advantage_per_token\n",
    "    adv2 = clipped_importance_ratio * advantage_per_token\n",
    "    policy_objective_per_token = torch.min(adv1, adv2) # [batch_size, num_generation, seq_len]\n",
    "    \n",
    "    mean_policy_objective = policy_objective_per_token.mean() # [1,]\n",
    "\n",
    "    # [batch_size, num_generation, seq_len]\n",
    "    kl_per_token = torch.exp(ref_logp_per_token - logp_per_token) - (ref_logp_per_token - logp_per_token) - 1\n",
    "    mean_kl = kl_per_token.mean()\n",
    "    \n",
    "    loss = beta * mean_kl - mean_policy_objective\n",
    "    \n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
